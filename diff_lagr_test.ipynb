{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIqU+Jiy3OZoYzBMDIWs70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rotogni/diffusion-lagr/blob/master/diff_lagr_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNbRj0xHZNDY"
      },
      "outputs": [],
      "source": [
        "# Set up environment for guided diffusion project\n",
        "!apt-get update\n",
        "!apt-get install -y libopenmpi-dev openmpi-bin\n",
        "!pip install mpi4py\n",
        "!pip install --no-binary=h5py h5py\n",
        "\n",
        "# Clone the repository (replace with the actual repo URL)\n",
        "!git clone https://github.com/rotogni/diffusion-lagr.git\n",
        "%cd diffusion-lagr\n",
        "\n",
        "# Install the package in development mode\n",
        "!pip install -e .\n",
        "\n",
        "# Verify installation\n",
        "!python -c \"import guided_diffusion; print('guided_diffusion installed successfully')\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the flags\n",
        "DATA_FLAGS=\"--dataset_path datasets/trajectories.h5 --dataset_name train\"\n",
        "MODEL_FLAGS=\"--dims 1 --image_size 295 --in_channels 3 --num_channels 128 --num_res_blocks 3 --attention_resolutions 250,125 --channel_mult 1,1,2,3,4\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 800 --noise_schedule tanh6,1\"\n",
        "TRAIN_FLAGS=\"--lr 1e-4 --batch_size 64\""
      ],
      "metadata": {
        "id": "xdUzWG_WaWzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training command\n",
        "!python scripts/turb_train.py $DATA_FLAGS $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ],
      "metadata": {
        "id": "oaf6UGwEafvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set sampling flags\n",
        "SAMPLE_FLAGS=\"--num_samples 200 --batch_size 64 --model_path ema_0.9999_250000.pt\"\n",
        "\n",
        "#Please note that the $MODEL_FLAGS and $DIFFUSION_FLAGS should be the same as those used in training."
      ],
      "metadata": {
        "id": "LGjhmW8Ea_mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling command\n",
        "!python scripts/turb_sample.py $SAMPLE_FLAGS $MODEL_FLAGS $DIFFUSION_FLAGS\n"
      ],
      "metadata": {
        "id": "kIljY7aaa5o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After sampling with the above command, it will generate a file named samples_179200x2000x3.npz (for DM-3c as an example).\n",
        "# You can use the following code to read and retrieve the generated velocities:\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File('datasets/trajectories.h5', 'r') as h5f:\n",
        "    rx0 = np.array(h5f.get('min'))\n",
        "    rx1 = np.array(h5f.get('max'))\n",
        "\n",
        "u3c = (np.load('samples_179200x2000x3.npz')['arr_0']+1)*(rx1-rx0)/2 + rx0\n",
        "#Just like for training, you can use multiple GPUs for sampling.\n"
      ],
      "metadata": {
        "id": "a7RNPQ9tbabI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}